{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch as t\n",
    "# import urllib.request\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hugging Face의 한국어 프리트레인 된 Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tokenizer, model 불러오고 정의하기\n",
    "2. 보낼 메세지와 행동 정의하기\n",
    "3. tokenizer를 통해 인풋할 메세지를 변환해주고 input_ids 변수에 넣기\n",
    "4. terminators로 마지막 문자 정의하기\n",
    "5. model.을 활용하여 응답 생성\n",
    "6. response 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6d0c5b2c964428bcf8008a9aa5082e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/Llama-3-Open-Ko-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"beomi/Llama-3-Open-Ko-8B\")\n",
    "\n",
    "# # 또는\n",
    "\n",
    "# model_id = \"beomi/Llama-3-Open-Ko-8B-Instruct-preview\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=\"auto\", # 적절한 데이터 타입과\n",
    "#     device_map=\"auto\", # 장치를 선택하도록 하는 코드 예) GPU\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇에게 보낼 메시지와 행동\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하자. 모든 대답은 한국어(Korean)으로 대답해줘.\"},\n",
    "    {\"role\": \"user\", \"content\": \"피보나치 수열이 뭐야? 그리고 피보나치 수열에 대해 파이썬 코드를 짜줘볼래?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지를 템플릿에 맞게 변환하고 토크나이저를 통해 텐서로 변환\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종료 토큰을 설정\n",
    "# 빈 문자열을 토큰 ID로 변환하여 종료 조건을 설정한다\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답 생성\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답 디코딩 출력\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로컬 다운로드 후 사용하는 기본 llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain을 통해 접근하는 방법은 별도의 tokenizer나 terminators가 필요하지 않음\n",
    "1. Langchain을 통해 임포트\n",
    "2. 모델 인스턴스 생성\n",
    "3. 응답 생성 함수 정의\n",
    "4. response 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = Ollama(model=\"llama3\", stop=[\"<|eot_id|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_response(user_prompt, system_prompt):\n",
    "  template = \"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    {system_prompt}\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    {user_prompt}\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id\n",
    "  \"\"\"\n",
    "  prompt = PromptTemplate(\n",
    "      template=template,\n",
    "      input_variables=[\"user_prompt\", \"system_prompt\"]\n",
    "  )\n",
    "\n",
    "  response = llm(prompt.format(system_prompt=system_prompt, user_prompt=user_prompt))\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_response(\"당신은 사랑받기 위해 태어난 사람\", \"노래가사 작성해주세요. 한국어로\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어로 프리트레인 된 Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='cmpl-c4cc9f85600b4378abf4215e3fdc8d7b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='정의라는 개념은 법률에서 매우 중요한 역할을 하며, 법령의 형성과 해석에 있어 필수적인 요소로 간주된다. 그러나 정의는 명확히 규정할 수 없는 개념이므로, 이를 둘러싼 해석의 논란이 항상 존재할 수 있다. 이러한 문제를 해결하기 위해, 사회구성원의 대다수가 납득할 수 있는 보편적 정의를 입증하는 방법을 찾아야 할 것이다. 이러한 방법을 찾기 위해, 우선 정의의 개념을 살펴보아야 하며, 이를 뒷받침하는 이론적 근거를 찾아야 한다. 또한, 정의의 개념을 실제 사회에서 적용하는 방법을 찾아야 하며, 이를 통해 사회구성원의 대다수가 납득할 수 있는 보편적 정의를 입증할 수 있는지 확인해야 한다. 이를 위해, 본 글에서는 정의의 개념을 살펴보고, 이를 뒷받침하는 이론적 근거를 찾아, 실제 사회에서 적용하는 방법을 찾아보아, 사회구성원의 대다수가 납득할 수 있는 보편적 정의를 입증하는 방법을 제시하고자 한다.', role='assistant', function_call=None, tool_calls=None), stop_reason=None)], created=1721192982, model='xionic-ko-llama-3-70b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=272, prompt_tokens=136, total_tokens=408))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"http://sionic.chat:8001/v1\",\n",
    "    api_key = \"934c4bbc-c384-4bea-af82-1450d7f8128d\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"xionic-ko-llama-3-70b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant. You will be given a task. You must generate a detailed and long answer in korean.\"},\n",
    "        {\"role\": \"user\", \"content\": \"각국 의 법률에서는 정의라는 개념이 자주 등장하며, 법령의 형성과 해석에 있어 매우 중요한 부분을 차지한다. 하지만 정의란 명>확히 규정 할 수 없는 개념이기에 해석의 논란 이 있을 수 있다. 그렇다면 사회구성원의 대다수가 납득할 수 있는 보편적 정의를 입증하는 방법은 무엇일지 생각해보아라.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print (response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
